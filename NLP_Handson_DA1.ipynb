{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"text-align: center;\"><span style=\"color: #000fff;\">Nagesh Buyre - 19MAI0024</span></h3>\n",
    "<h1 style=\"text-align: center;\">Natural Language Processing</h1>\n",
    "<h1 style=\"text-align: center;\">PyTorch.NLP and SpaCy</h1>\n",
    "<h3 style=\"text-align: center;\">Digital Assignment (22_June)</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we are talking about the NLP tool using PyTorvh. It has its own modules\n",
    "Let's discuss about the some of them "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bag-of-Words Text Classification\n",
    "In this tutorial we will show how to build a simple Bag of Words (BoW) text classifier using PyTorch. The classifier is trained on tweeter dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in c:\\users\\buyre\\anaconda3\\lib\\site-packages (1.3.1+cpu)\n",
      "Requirement already satisfied: numpy in c:\\users\\buyre\\anaconda3\\lib\\site-packages (from torch) (1.18.1)\n"
     ]
    }
   ],
   "source": [
    "#let's install torch\n",
    "!pip install torch\n",
    "#and it is already available in my system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import some importand libraries as well as the some torch libraries\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from tqdm import tqdm, tqdm_notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "\n",
    "tweets_data = pd.read_csv('C:/Users/buyre/Internship_Ureka/WEEK3/train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_data = tweets_data.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(27447, 2)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>oh Marly, I`m so sorry!!  I hope you find her...</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Playing Ghost Online is really interesting. Th...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>is cleaning the house for her family who is co...</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>gotta restart my computer .. I thought Win7 wa...</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>SEe waT I Mean bOuT FoLL0w fRiiDaYs... It`S cA...</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text sentiment\n",
       "0   oh Marly, I`m so sorry!!  I hope you find her...   neutral\n",
       "1  Playing Ghost Online is really interesting. Th...  positive\n",
       "2  is cleaning the house for her family who is co...   neutral\n",
       "3  gotta restart my computer .. I thought Win7 wa...   neutral\n",
       "4  SEe waT I Mean bOuT FoLL0w fRiiDaYs... It`S cA...   neutral"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#vectorizarization\n",
    "data_vectorizer = CountVectorizer(stop_words='english', max_df=0.99, min_df=0.005)\n",
    "\n",
    "#dataframes\n",
    "data_sequences = data_vectorizer.fit_transform(tweets_data.text.tolist())\n",
    "data_labels = tweets_data.sentiment.tolist()\n",
    "\n",
    "#tokenization using vectorization\n",
    "data_token2idx = data_vectorizer.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'oh': 117,\n",
       " 'sorry': 141,\n",
       " 'hope': 68,\n",
       " 'soon': 140,\n",
       " 'really': 130,\n",
       " 'new': 114,\n",
       " 'job': 75,\n",
       " 'wait': 174,\n",
       " 'house': 71,\n",
       " 'family': 34,\n",
       " 'later': 79,\n",
       " 'today': 163,\n",
       " 'gotta': 52,\n",
       " 'thought': 158,\n",
       " 'friday': 39,\n",
       " 'fun': 42,\n",
       " 'im': 74,\n",
       " 'way': 180,\n",
       " 'omg': 120,\n",
       " 'went': 184,\n",
       " 'sleep': 138,\n",
       " 'working': 188,\n",
       " 'going': 47,\n",
       " 'home': 67,\n",
       " 'twitter': 170,\n",
       " 'make': 98,\n",
       " 'got': 51,\n",
       " 'gonna': 49,\n",
       " 'try': 166,\n",
       " 'best': 8,\n",
       " 'watch': 178,\n",
       " 'tomorrow': 164,\n",
       " 'play': 124,\n",
       " 'thats': 153,\n",
       " 'http': 72,\n",
       " 'bit': 12,\n",
       " 'ly': 97,\n",
       " 'miss': 103,\n",
       " 'just': 76,\n",
       " 'coming': 20,\n",
       " 'day': 22,\n",
       " 'school': 136,\n",
       " 'little': 85,\n",
       " 'happy': 58,\n",
       " 'ok': 118,\n",
       " 'time': 160,\n",
       " 'love': 95,\n",
       " 'car': 14,\n",
       " 'big': 10,\n",
       " 'waiting': 175,\n",
       " 'thing': 154,\n",
       " 'like': 84,\n",
       " 'bad': 6,\n",
       " 'look': 90,\n",
       " 'help': 64,\n",
       " 'girl': 44,\n",
       " 'hair': 57,\n",
       " 'old': 119,\n",
       " 'feel': 35,\n",
       " 'wanna': 176,\n",
       " 'work': 187,\n",
       " 'dont': 30,\n",
       " 'looking': 91,\n",
       " 'week': 182,\n",
       " 'check': 16,\n",
       " 'com': 18,\n",
       " 'hate': 60,\n",
       " 'bored': 13,\n",
       " 'guess': 54,\n",
       " 'days': 23,\n",
       " 'early': 31,\n",
       " 'morning': 108,\n",
       " 'late': 78,\n",
       " 'hurts': 73,\n",
       " 'lost': 93,\n",
       " 'friends': 41,\n",
       " 'haha': 56,\n",
       " 'yes': 195,\n",
       " 'better': 9,\n",
       " 'let': 82,\n",
       " 'nice': 115,\n",
       " 'awesome': 3,\n",
       " 'good': 50,\n",
       " 'friend': 40,\n",
       " 'yay': 192,\n",
       " 'tonight': 165,\n",
       " 'right': 131,\n",
       " 'know': 77,\n",
       " 'don': 29,\n",
       " 'think': 156,\n",
       " 'lol': 88,\n",
       " 'summer': 146,\n",
       " 'yesterday': 196,\n",
       " 'said': 133,\n",
       " 'say': 135,\n",
       " 'hi': 66,\n",
       " 'things': 155,\n",
       " 'll': 87,\n",
       " 'head': 62,\n",
       " 'wow': 190,\n",
       " 'tired': 162,\n",
       " 'hey': 65,\n",
       " 'didn': 25,\n",
       " 'tell': 150,\n",
       " 'thank': 151,\n",
       " 'sick': 137,\n",
       " 'guys': 55,\n",
       " 'star': 142,\n",
       " 'enjoy': 32,\n",
       " 'left': 81,\n",
       " 'wish': 185,\n",
       " 'ready': 129,\n",
       " 'cool': 21,\n",
       " 'doing': 28,\n",
       " 'getting': 43,\n",
       " 'mothers': 110,\n",
       " 'gone': 48,\n",
       " 'cause': 15,\n",
       " 'read': 128,\n",
       " 'bed': 7,\n",
       " 'night': 116,\n",
       " 'saw': 134,\n",
       " 'pretty': 126,\n",
       " 'thanks': 152,\n",
       " 'song': 139,\n",
       " 'leave': 80,\n",
       " 'aww': 4,\n",
       " 'sad': 132,\n",
       " 'want': 177,\n",
       " 'making': 100,\n",
       " 'weather': 181,\n",
       " 'sun': 147,\n",
       " 'come': 19,\n",
       " 'hours': 70,\n",
       " 'poor': 125,\n",
       " 'watching': 179,\n",
       " 'music': 112,\n",
       " 'monday': 107,\n",
       " 'need': 113,\n",
       " 'mother': 109,\n",
       " 'mom': 106,\n",
       " 'people': 122,\n",
       " 'missed': 104,\n",
       " 'movie': 111,\n",
       " 'actually': 0,\n",
       " 'sucks': 145,\n",
       " 'tho': 157,\n",
       " 'lunch': 96,\n",
       " 'did': 24,\n",
       " 'yeah': 193,\n",
       " 'feeling': 36,\n",
       " 'god': 46,\n",
       " 'maybe': 102,\n",
       " 'great': 53,\n",
       " 'excited': 33,\n",
       " 'having': 61,\n",
       " 'tweet': 168,\n",
       " 'life': 83,\n",
       " 'finally': 37,\n",
       " 'baby': 5,\n",
       " 'party': 121,\n",
       " 'year': 194,\n",
       " 'hear': 63,\n",
       " 'talk': 149,\n",
       " 'missing': 105,\n",
       " 'twitpic': 169,\n",
       " 've': 173,\n",
       " 'trying': 167,\n",
       " 'live': 86,\n",
       " 'hard': 59,\n",
       " 'world': 189,\n",
       " 'looks': 92,\n",
       " 'lot': 94,\n",
       " 'long': 89,\n",
       " 'does': 26,\n",
       " 'amazing': 1,\n",
       " 'birthday': 11,\n",
       " 'glad': 45,\n",
       " 'makes': 99,\n",
       " 'till': 159,\n",
       " 'weekend': 183,\n",
       " 'ur': 172,\n",
       " 'follow': 38,\n",
       " 'sure': 148,\n",
       " 'doesn': 27,\n",
       " 'start': 143,\n",
       " 'ya': 191,\n",
       " 'tinyurl': 161,\n",
       " 'phone': 123,\n",
       " 'away': 2,\n",
       " 'won': 186,\n",
       " 'man': 101,\n",
       " 'hot': 69,\n",
       " 'ugh': 171,\n",
       " 'stuff': 144,\n",
       " 'cold': 17,\n",
       " 'rain': 127}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#TOKENIZATION\n",
    "data_token2idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BagOfWordsClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size, hidden1, hidden2):\n",
    "        super(BagOfWordsClassifier, self).__init__()\n",
    "        self.fc1 = nn.Linear(vocab_size, hidden1)\n",
    "        self.fc2 = nn.Linear(hidden1, hidden2)\n",
    "        self.fc3 = nn.Linear(hidden2, 1)\n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        x = F.relu(self.fc1(inputs.squeeze(1).float()))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        return self.fc3(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BagOfWordsClassifier(\n",
       "  (fc1): Linear(in_features=197, out_features=128, bias=True)\n",
       "  (fc2): Linear(in_features=128, out_features=64, bias=True)\n",
       "  (fc3): Linear(in_features=64, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = BagOfWordsClassifier(len(data_idx2token), 128, 64)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.Adam([p for p in model.parameters() if p.requires_grad], lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(tweets_data, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torchtext\n",
      "  Downloading torchtext-0.6.0-py3-none-any.whl (64 kB)\n",
      "Requirement already satisfied: six in c:\\users\\buyre\\anaconda3\\lib\\site-packages (from torchtext) (1.14.0)\n",
      "Requirement already satisfied: torch in c:\\users\\buyre\\anaconda3\\lib\\site-packages (from torchtext) (1.3.1+cpu)\n",
      "Requirement already satisfied: requests in c:\\users\\buyre\\anaconda3\\lib\\site-packages (from torchtext) (2.22.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\buyre\\anaconda3\\lib\\site-packages (from torchtext) (1.18.1)\n",
      "Requirement already satisfied: sentencepiece in c:\\users\\buyre\\anaconda3\\lib\\site-packages (from torchtext) (0.1.91)\n",
      "Requirement already satisfied: tqdm in c:\\users\\buyre\\anaconda3\\lib\\site-packages (from torchtext) (4.42.1)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in c:\\users\\buyre\\anaconda3\\lib\\site-packages (from requests->torchtext) (1.25.8)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\buyre\\anaconda3\\lib\\site-packages (from requests->torchtext) (2020.4.5.1)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in c:\\users\\buyre\\anaconda3\\lib\\site-packages (from requests->torchtext) (3.0.4)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in c:\\users\\buyre\\anaconda3\\lib\\site-packages (from requests->torchtext) (2.8)\n",
      "Installing collected packages: torchtext\n",
      "Successfully installed torchtext-0.6.0\n"
     ]
    }
   ],
   "source": [
    "!pip install torchtext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers\n",
      "  Downloading transformers-2.11.0-py3-none-any.whl (674 kB)\n",
      "Collecting sacremoses\n",
      "  Downloading sacremoses-0.0.43.tar.gz (883 kB)\n",
      "Collecting regex!=2019.12.17\n",
      "  Downloading regex-2020.6.8-cp37-cp37m-win_amd64.whl (268 kB)\n",
      "Requirement already satisfied: requests in c:\\users\\buyre\\anaconda3\\lib\\site-packages (from transformers) (2.22.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\buyre\\anaconda3\\lib\\site-packages (from transformers) (1.18.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\buyre\\anaconda3\\lib\\site-packages (from transformers) (4.42.1)\n",
      "Requirement already satisfied: packaging in c:\\users\\buyre\\anaconda3\\lib\\site-packages (from transformers) (20.1)\n",
      "Requirement already satisfied: sentencepiece in c:\\users\\buyre\\anaconda3\\lib\\site-packages (from transformers) (0.1.91)\n",
      "Collecting tokenizers==0.7.0\n",
      "  Downloading tokenizers-0.7.0-cp37-cp37m-win_amd64.whl (1.1 MB)\n",
      "Requirement already satisfied: filelock in c:\\users\\buyre\\anaconda3\\lib\\site-packages (from transformers) (3.0.12)\n",
      "Requirement already satisfied: six in c:\\users\\buyre\\anaconda3\\lib\\site-packages (from sacremoses->transformers) (1.14.0)\n",
      "Requirement already satisfied: click in c:\\users\\buyre\\anaconda3\\lib\\site-packages (from sacremoses->transformers) (7.0)\n",
      "Requirement already satisfied: joblib in c:\\users\\buyre\\anaconda3\\lib\\site-packages (from sacremoses->transformers) (0.14.1)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in c:\\users\\buyre\\anaconda3\\lib\\site-packages (from requests->transformers) (1.25.8)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in c:\\users\\buyre\\anaconda3\\lib\\site-packages (from requests->transformers) (3.0.4)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in c:\\users\\buyre\\anaconda3\\lib\\site-packages (from requests->transformers) (2.8)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\buyre\\anaconda3\\lib\\site-packages (from requests->transformers) (2020.4.5.1)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in c:\\users\\buyre\\anaconda3\\lib\\site-packages (from packaging->transformers) (2.4.6)\n",
      "Building wheels for collected packages: sacremoses\n",
      "  Building wheel for sacremoses (setup.py): started\n",
      "  Building wheel for sacremoses (setup.py): finished with status 'done'\n",
      "  Created wheel for sacremoses: filename=sacremoses-0.0.43-py3-none-any.whl size=893262 sha256=f7b85d54abd30c39328b6a3b82e02f03f4b4def7454bc12c5f37f72bda2f09e5\n",
      "  Stored in directory: c:\\users\\buyre\\appdata\\local\\pip\\cache\\wheels\\69\\09\\d1\\bf058f7d6fa0ecba2ce7c66be3b8d012beb4bf61a6e0c101c0\n",
      "Successfully built sacremoses\n",
      "Installing collected packages: regex, sacremoses, tokenizers, transformers\n",
      "Successfully installed regex-2020.6.8 sacremoses-0.0.43 tokenizers-0.7.0 transformers-2.11.0\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: en_core_web_sm==2.2.5 from https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.2.5/en_core_web_sm-2.2.5.tar.gz#egg=en_core_web_sm==2.2.5 in c:\\users\\buyre\\anaconda3\\lib\\site-packages (2.2.5)\n",
      "Requirement already satisfied: spacy>=2.2.2 in c:\\users\\buyre\\anaconda3\\lib\\site-packages (from en_core_web_sm==2.2.5) (2.2.4)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\buyre\\anaconda3\\lib\\site-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.2)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\buyre\\anaconda3\\lib\\site-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (2.22.0)\n",
      "Requirement already satisfied: thinc==7.4.0 in c:\\users\\buyre\\anaconda3\\lib\\site-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (7.4.0)\n",
      "Requirement already satisfied: plac<1.2.0,>=0.9.6 in c:\\users\\buyre\\anaconda3\\lib\\site-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.1.3)\n",
      "Requirement already satisfied: blis<0.5.0,>=0.4.0 in c:\\users\\buyre\\anaconda3\\lib\\site-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (0.4.1)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\buyre\\anaconda3\\lib\\site-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (3.0.2)\n",
      "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in c:\\users\\buyre\\anaconda3\\lib\\site-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.2)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\buyre\\anaconda3\\lib\\site-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (4.42.1)\n",
      "Requirement already satisfied: numpy>=1.15.0 in c:\\users\\buyre\\anaconda3\\lib\\site-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.18.1)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in c:\\users\\buyre\\anaconda3\\lib\\site-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (0.6.0)\n",
      "Requirement already satisfied: setuptools in c:\\users\\buyre\\anaconda3\\lib\\site-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (45.2.0.post20200210)\n",
      "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in c:\\users\\buyre\\anaconda3\\lib\\site-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.0)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\buyre\\anaconda3\\lib\\site-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (2.0.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\buyre\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (2020.4.5.1)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in c:\\users\\buyre\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (1.25.8)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in c:\\users\\buyre\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.0.4)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in c:\\users\\buyre\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (2.8)\n",
      "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in c:\\users\\buyre\\anaconda3\\lib\\site-packages (from catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (1.5.0)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\users\\buyre\\anaconda3\\lib\\site-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (2.2.0)\n",
      "[+] Download and installation successful\n",
      "You can now load the model via spacy.load('en_core_web_sm')\n",
      "[x] Couldn't link model to 'en'\n",
      "Creating a symlink in spacy/data failed. Make sure you have the required\n",
      "permissions and try re-running the command as admin, or use a virtualenv. You\n",
      "can still import the model as a module and call its load() method, or create the\n",
      "symlink manually.\n",
      "C:\\Users\\buyre\\Anaconda3\\lib\\site-packages\\en_core_web_sm -->\n",
      "C:\\Users\\buyre\\Anaconda3\\lib\\site-packages\\spacy\\data\\en\n",
      "[!] Download successful but linking failed\n",
      "Creating a shortcut link for 'en' didn't work (maybe you don't have admin\n",
      "permissions?), but you can still load the model via its full package name: nlp =\n",
      "spacy.load('en_core_web_sm')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You do not have sufficient privilege to perform this operation.\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy download en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.0674,  1.2496, -0.7283,  1.0215, -0.4966]],\n",
      "       grad_fn=<EmbeddingBackward>)\n"
     ]
    }
   ],
   "source": [
    "word_to_ix = {\"hello\": 0, \"world\": 1}\n",
    "embeds = nn.Embedding(2, 5)  # 2 words in vocab, 5 dimensional embeddings\n",
    "lookup_tensor = torch.tensor([word_to_ix[\"hello\"]], dtype=torch.long)\n",
    "hello_embed = embeds(lookup_tensor)\n",
    "print(hello_embed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
